{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1mFqVE_52xKMaDUrAE8M8vPCnGzS0vdvV","authorship_tag":"ABX9TyO8bsg83R1eSN+hi2G73dNe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UJxsXNr610PW"},"outputs":[],"source":["import numpy as np\n","\n","from tqdm import tqdm, trange\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","\n","from torchvision.transforms import ToTensor\n","from torchvision.datasets.mnist import MNIST\n","\n","np.random.seed(0)\n","torch.manual_seed(0)\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import os.path\n","import cv2\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["def patchify(images, n_patches):\n","    n, c, h, w = images.shape\n","\n","    assert h == w, \"Patchify method is implemented for square images only\"\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n","    patch_size = h // n_patches\n","\n","    for idx, image in enumerate(images):\n","        for i in range(n_patches):\n","            for j in range(n_patches):\n","                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","    return patches"],"metadata":{"id":"QQI7M5GH1_Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class MyMSA(nn.Module):\n","    def __init__(self, d, n_heads=2):\n","        super(MyMSA, self).__init__()\n","        self.d = d\n","        self.n_heads = n_heads\n","\n","        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n","\n","        d_head = int(d / n_heads)\n","        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.d_head = d_head\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, sequences):\n","        # Sequences has shape (N, seq_length, token_dim)\n","        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n","        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","\n","                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n","                seq_result.append(attention @ v)\n","            result.append(torch.hstack(seq_result))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"],"metadata":{"id":"Fn_YcIae2AJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class MyViTBlock(nn.Module):\n","    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n","        super(MyViTBlock, self).__init__()\n","        self.hidden_d = hidden_d\n","        self.n_heads = n_heads\n","\n","        self.norm1 = nn.LayerNorm(hidden_d)\n","        self.mhsa = MyMSA(hidden_d, n_heads)\n","        self.norm2 = nn.LayerNorm(hidden_d)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n","        )\n","\n","    def forward(self, x):\n","        out = x + self.mhsa(self.norm1(x))\n","        out = out + self.mlp(self.norm2(out))\n","        return out"],"metadata":{"id":"fu385jlH2CGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class MyViT(nn.Module):\n","    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n","        # Super constructor\n","        super(MyViT, self).__init__()\n","\n","        # Attributes\n","        self.chw = chw # ( C , H , W )\n","        self.n_patches = n_patches\n","        self.n_blocks = n_blocks\n","        self.n_heads = n_heads\n","        self.hidden_d = hidden_d\n","\n","        # Input and patches sizes\n","        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n","        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n","\n","        # 1) Linear mapper\n","        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n","\n","        # 2) Learnable classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n","\n","        # 3) Positional embedding\n","        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n","\n","        # 4) Transformer encoder blocks\n","        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n","\n","        # 5) Classification MLPk\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.hidden_d, out_d),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        # Dividing images into patches\n","        n, c, h, w = images.shape\n","        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n","\n","        # Running linear layer tokenization\n","        # Map the vector corresponding to each patch to the hidden size dimension\n","        tokens = self.linear_mapper(patches)\n","\n","        # Adding classification token to the tokens\n","        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n","\n","        # Adding positional embedding\n","        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n","\n","        # Transformer Blocksx\n","        for block in self.blocks:\n","            out = block(out)\n","\n","        # Getting the classification token only\n","        out = out[:, 0]\n","\n","        return self.mlp(out) # Map to output dimension, output category distribution\n","\n","\n"],"metadata":{"id":"U_W9BO542FnI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def get_positional_embeddings(sequence_length, d):\n","    result = torch.ones(sequence_length, d)\n","    for i in range(sequence_length):\n","        for j in range(d):\n","            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n","    return result\n","\n"],"metadata":{"id":"L82J3AOv2GtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/축산물이미지데이터와라벨정보_샘플데이터/원천데이터/1. 소도제/1.segmentation\")\n","\n","filepaths = pd.Series(sorted(list(image_dir.glob(r'**/*.jpg'))), name='Filepath').astype(str)\n","# print(filepaths[1])\n","\n","def extract_id(filepath):\n","    segments = filepath.split('/')\n","    # Get the last segment\n","    filename = segments[-1]\n","    # Split the filename based on '_'\n","    filename_parts = filename.split('_')\n","    # Extract the ID from the filename\n","    id = filename_parts[-1].split('.')[0]\n","    return id\n","\n","id = pd.Series(filepaths.map(extract_id), name='Id').astype(int)\n","print(id)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbYAAOsl2H50","executionInfo":{"status":"ok","timestamp":1686645348643,"user_tz":-540,"elapsed":4,"user":{"displayName":"박지섭","userId":"11788625731049142912"}},"outputId":"9fe06d25-e8f6-4ec4-aae6-561a7be8a4a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/축산물이미지데이터와라벨정보_샘플데이터/원천데이터/1. 소도제/1.segmentation/1/QC_cow_segmentation_1_000014.jpg\n","0          6\n","1         14\n","2         18\n","3         22\n","4         30\n","       ...  \n","995    60250\n","996    60252\n","997    60265\n","998    60270\n","999    60277\n","Name: Id, Length: 1000, dtype: int64\n"]}]},{"cell_type":"code","source":["import json\n","from pandas import json_normalize\n","\n","label_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/축산물이미지데이터와라벨정보_샘플데이터/라벨링데이터/1.소도체\")\n","\n","# Get the file paths of JSON files in the directory\n","json_files = list(label_dir.glob(\"*.json\"))\n","\n","# Create an empty list to store the DataFrames\n","dfs = []\n","\n","# Iterate over each JSON file\n","for file_path in json_files:\n","    # Read JSON data from the file\n","    with open(file_path) as file:\n","        json_data = json.load(file)\n","\n","    # Extract the required information\n","    file_id = file_path.stem.split('_')[-1]  # Extract the last ID from the file name\n","    grade = json_data['label_info']['shapes'][0]['grade']\n","    gender = json_data['label_info']['shapes'][0]['gender']\n","    width = json_data['label_info']['image']['width']\n","    height = json_data['label_info']['image']['height']\n","\n","    # Create a DataFrame\n","    df = pd.DataFrame({\n","        'Filepath': filepaths[filepaths.str.contains(file_id)],  # Filter the matching file path based on the file ID\n","        'grade': [grade],\n","        'gender': [gender],\n","        'width': [width],\n","        'height': [height]\n","    })\n","\n","    # Append the DataFrame to the list\n","    dfs.append(df)\n","\n","# Concatenate all DataFrames into a single DataFrame\n","combined_df = pd.concat(dfs, ignore_index=True)\n","\n","# Print the combined DataFrame\n","print(combined_df)\n","print(combined_df.groupby('gender').count())\n","print(combined_df.groupby('grade').count())\n","print(combined_df.columns)\n","\n","df = combined_df.drop(['grade', 'width', 'height'], inplace = True, axis = 1)\n","print(combined_df.columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VWMmETc20dV","executionInfo":{"status":"ok","timestamp":1686647771536,"user_tz":-540,"elapsed":4015,"user":{"displayName":"박지섭","userId":"11788625731049142912"}},"outputId":"922bfe78-6a93-4023-ad59-903e8d239e25"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              Filepath grade  gender  width  \\\n","0    /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1  female   1080   \n","1    /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1   steer   1080   \n","2    /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1  female   1080   \n","3    /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1  female   1080   \n","4    /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1   steer   1080   \n","..                                                 ...   ...     ...    ...   \n","995  /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1   steer   1080   \n","996  /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1   steer   1080   \n","997  /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1   steer   1080   \n","998  /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1   steer   1080   \n","999  /content/drive/MyDrive/Colab Notebooks/축산ᄆ...     1  female   1080   \n","\n","     height  \n","0      1920  \n","1      1920  \n","2      1920  \n","3      1920  \n","4      1920  \n","..      ...  \n","995    1920  \n","996    1920  \n","997    1920  \n","998    1920  \n","999    1920  \n","\n","[1000 rows x 5 columns]\n","        Filepath  grade  width  height\n","gender                                \n","female       217    217    217     217\n","steer        783    783    783     783\n","       Filepath  gender  width  height\n","grade                                 \n","1          1000    1000   1000    1000\n","Index(['Filepath', 'grade', 'gender', 'width', 'height'], dtype='object')\n","Index(['Filepath', 'gender'], dtype='object')\n"]}]},{"cell_type":"code","source":["\n","def main():\n","    # Loading data\n","    transform = ToTensor()\n","\n","    train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n","    test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n","\n","    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n","    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n","\n","    # Defining model and training options\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n","    model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n","    N_EPOCHS = 5\n","    LR = 0.005\n","\n","    # Training loop\n","    optimizer = Adam(model.parameters(), lr=LR)\n","    criterion = CrossEntropyLoss()\n","    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n","        train_loss = 0.0\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            loss = criterion(y_hat, y)\n","\n","            train_loss += loss.detach().cpu().item() / len(train_loader)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n","\n","    # Test loop\n","    with torch.no_grad():\n","        correct, total = 0, 0\n","        test_loss = 0.0\n","        for batch in tqdm(test_loader, desc=\"Testing\"):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = model(x)\n","            loss = criterion(y_hat, y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","\n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            total += len(x)\n","        print(f\"Test loss: {test_loss:.2f}\")\n","        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n","\n"],"metadata":{"id":"D2A1_kUQ5m1X"},"execution_count":null,"outputs":[]}]}